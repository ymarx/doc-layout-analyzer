#!/usr/bin/env python3
"""
ì¢…í•© íŒŒì´í”„ë¼ì¸ ê±´ì „ì„± í…ŒìŠ¤íŠ¸
ë¦¬íŒ©í† ë§ëœ ì‹œìŠ¤í…œì˜ ê° ë‹¨ê³„ë³„ ëª¨ë“ˆ ì‘ë™ ë° ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
"""

import asyncio
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# ë¦¬íŒ©í† ë§ëœ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸
from src.core.simplified_config import (
    ProcessingLevel, PipelineConfig, SimplifiedConfigManager,
    create_basic_config, create_standard_config, create_complete_config
)
from src.core.modernized_pipeline import ModernizedPipeline, quick_process
from src.parsers.unified_docx_parser import UnifiedDocxParser
from src.core.template_manager import TemplateManager
from src.core.user_annotations import UserAnnotationManager


class ComprehensivePipelineTest:
    """ì¢…í•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

    def __init__(self, output_dir: str = "test_pipeline_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.test_results = {}
        self.start_time = time.time()

    async def run_full_test(self, document_path: str = None):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ”¬ ì¢…í•© íŒŒì´í”„ë¼ì¸ ê±´ì „ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 70)

        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸
        test_document = self._find_test_document(document_path)
        if not test_document:
            return await self._create_mock_test()

        print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {test_document}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print()

        # ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await self._test_1_individual_modules(test_document)
        await self._test_2_parser_functionality(test_document)
        await self._test_3_configuration_system()
        await self._test_4_pipeline_stages(test_document)
        await self._test_5_end_to_end_processing(test_document)
        await self._test_6_output_quality_assessment()

        # ì¢…í•© í‰ê°€
        self._generate_comprehensive_report()

    def _find_test_document(self, document_path: str) -> str:
        """í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì°¾ê¸°"""
        if document_path and Path(document_path).exists():
            return document_path

        # ê°€ëŠ¥í•œ ìœ„ì¹˜ë“¤ í™•ì¸
        possible_paths = [
            "ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx",
            "../ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx",
            "../../ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx",
            "test_document.docx",
            "sample.docx"
        ]

        for path in possible_paths:
            if Path(path).exists():
                return path

        return None

    async def _create_mock_test(self):
        """í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ì„ ë•Œ ëª¨ì˜ í…ŒìŠ¤íŠ¸"""
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ëª¨ì˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤")
        print("ì‹¤ì œ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ 'ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx' íŒŒì¼ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
        print()

        # ëª¨ì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
        self.test_results = {
            "mock_test": True,
            "individual_modules": {"status": "âœ… PASS", "note": "ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"},
            "parser_functionality": {"status": "âš ï¸ SKIP", "note": "ì‹¤ì œ ë¬¸ì„œ í•„ìš”"},
            "configuration_system": {"status": "âœ… PASS", "note": "ì„¤ì • ì‹œìŠ¤í…œ ì •ìƒ"},
            "pipeline_stages": {"status": "âš ï¸ SKIP", "note": "ì‹¤ì œ ë¬¸ì„œ í•„ìš”"},
            "end_to_end": {"status": "âš ï¸ SKIP", "note": "ì‹¤ì œ ë¬¸ì„œ í•„ìš”"},
            "output_quality": {"status": "âš ï¸ SKIP", "note": "ì¶œë ¥ë¬¼ ì—†ìŒ"}
        }

        await self._test_3_configuration_system()
        print("\nğŸ¯ ëª¨ì˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("ì‹¤ì œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œëŠ” ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    async def _test_1_individual_modules(self, document_path: str):
        """1ë‹¨ê³„: ê°œë³„ ëª¨ë“ˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§© 1ë‹¨ê³„: ê°œë³„ ëª¨ë“ˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        module_tests = {}

        try:
            # í†µí•© íŒŒì„œ í…ŒìŠ¤íŠ¸
            parser = UnifiedDocxParser()
            module_tests["unified_parser"] = {
                "initialized": True,
                "supports_docx": parser.can_handle(document_path),
                "parsing_modes": len(parser.parsing_modes)
            }
            print("âœ… UnifiedDocxParser: ì´ˆê¸°í™” ë° ê¸°ëŠ¥ í™•ì¸ ì™„ë£Œ")

            # ì„¤ì • ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
            config_manager = SimplifiedConfigManager()
            module_tests["config_manager"] = {
                "initialized": True,
                "presets_count": len(config_manager.presets),
                "current_level": config_manager.current_preset.level.value
            }
            print("âœ… SimplifiedConfigManager: ì„¤ì • ì‹œìŠ¤í…œ ì •ìƒ")

            # í…œí”Œë¦¿ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
            template_manager = TemplateManager(self.output_dir / "templates")
            module_tests["template_manager"] = {
                "initialized": True,
                "templates_dir_exists": template_manager.templates_dir.exists()
            }
            print("âœ… TemplateManager: ì´ˆê¸°í™” ì™„ë£Œ")

            # ì£¼ì„ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
            annotation_manager = UserAnnotationManager(self.output_dir / "annotations")
            module_tests["annotation_manager"] = {
                "initialized": True,
                "annotations_path_exists": annotation_manager.annotations_path.exists()
            }
            print("âœ… UserAnnotationManager: ì´ˆê¸°í™” ì™„ë£Œ")

            self.test_results["individual_modules"] = {
                "status": "âœ… PASS",
                "details": module_tests,
                "passed_modules": len([m for m in module_tests.values() if m.get("initialized")])
            }

        except Exception as e:
            print(f"âŒ ê°œë³„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["individual_modules"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    async def _test_2_parser_functionality(self, document_path: str):
        """2ë‹¨ê³„: íŒŒì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”§ 2ë‹¨ê³„: íŒŒì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        try:
            parser = UnifiedDocxParser()

            # ê° íŒŒì‹± ëª¨ë“œ í…ŒìŠ¤íŠ¸
            parsing_results = {}

            for mode_name, mode_config in parser.parsing_modes.items():
                try:
                    print(f"   ğŸ§ª {mode_name} ëª¨ë“œ í…ŒìŠ¤íŠ¸...")

                    # ëª¨ì˜ íŒŒì‹± ì˜µì…˜ ìƒì„±
                    from src.parsers.base_parser import ProcessingOptions
                    options = ProcessingOptions()
                    options.parsing_complexity = mode_name

                    start_time = time.time()
                    result = await parser.parse(document_path, options)
                    parse_time = time.time() - start_time

                    parsing_results[mode_name] = {
                        "success": result.success,
                        "processing_time": parse_time,
                        "has_content": result.content is not None,
                        "docjson_generated": 'docjson' in (result.content or {}),
                        "error": result.error
                    }

                    status = "âœ…" if result.success else "âŒ"
                    print(f"   {status} {mode_name}: {parse_time:.3f}ì´ˆ")

                except Exception as e:
                    parsing_results[mode_name] = {
                        "success": False,
                        "error": str(e)
                    }
                    print(f"   âŒ {mode_name}: {e}")

            # íŒŒì„œ ì„±ëŠ¥ í‰ê°€
            successful_modes = len([r for r in parsing_results.values() if r.get("success")])
            total_modes = len(parsing_results)

            self.test_results["parser_functionality"] = {
                "status": "âœ… PASS" if successful_modes > 0 else "âŒ FAIL",
                "successful_modes": f"{successful_modes}/{total_modes}",
                "details": parsing_results,
                "best_performance": min([r.get("processing_time", 999) for r in parsing_results.values() if r.get("success")], default=0)
            }

            print(f"ğŸ“Š íŒŒì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {successful_modes}/{total_modes} ëª¨ë“œ ì„±ê³µ")

        except Exception as e:
            print(f"âŒ íŒŒì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["parser_functionality"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    async def _test_3_configuration_system(self):
        """3ë‹¨ê³„: ì„¤ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("âš™ï¸ 3ë‹¨ê³„: ì„¤ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        try:
            config_tests = {}

            # ê° ì²˜ë¦¬ ë ˆë²¨ í…ŒìŠ¤íŠ¸
            levels = [ProcessingLevel.BASIC, ProcessingLevel.STANDARD, ProcessingLevel.COMPLETE]

            for level in levels:
                config = PipelineConfig(processing_level=level)
                config_manager = SimplifiedConfigManager()
                preset = config.to_preset(config_manager)

                config_tests[level.value] = {
                    "level": level.value,
                    "description": preset.description,
                    "template_matching": preset.enable_template_matching,
                    "auto_annotations": preset.enable_auto_annotations,
                    "vectorization": preset.enable_vectorization,
                    "output_formats": preset.output_formats
                }

                print(f"   âœ… {level.value.upper()}: {preset.description}")

            # í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
            convenience_configs = {
                "basic": create_basic_config(),
                "standard": create_standard_config(),
                "complete": create_complete_config()
            }

            for name, config in convenience_configs.items():
                print(f"   âœ… {name}_config(): {config.processing_level.value}")

            # ë§ˆì´ê·¸ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸
            from src.core.simplified_config import migrate_legacy_config
            legacy_config = {
                'processing_mode': 'enhanced',
                'template_confidence_threshold': 0.7,
                'output_formats': ['docjson', 'annotations']
            }
            migrated = migrate_legacy_config(legacy_config)
            print(f"   âœ… ë ˆê±°ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜: {legacy_config['processing_mode']} â†’ {migrated.processing_level.value}")

            self.test_results["configuration_system"] = {
                "status": "âœ… PASS",
                "levels_tested": len(levels),
                "convenience_functions": len(convenience_configs),
                "migration_support": True,
                "details": config_tests
            }

        except Exception as e:
            print(f"âŒ ì„¤ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["configuration_system"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    async def _test_4_pipeline_stages(self, document_path: str):
        """4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸"""
        print("ğŸ­ 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        try:
            pipeline = ModernizedPipeline(output_dir=str(self.output_dir))
            config = create_standard_config()

            # ê°œë³„ ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜
            stage_results = {}

            # ëª¨ì˜ ê²°ê³¼ ê°ì²´ ìƒì„±
            from src.core.modernized_pipeline import ModernPipelineResult
            mock_result = ModernPipelineResult(
                success=False,
                document_id="test_doc",
                processing_level=ProcessingLevel.STANDARD
            )

            # Stage 1: Document Parsing í…ŒìŠ¤íŠ¸
            try:
                preset = config.to_preset(pipeline.config_manager)
                await pipeline._stage_document_parsing(document_path, preset, mock_result)
                stage_results["parsing"] = {
                    "success": "parsing" in mock_result.stages_completed,
                    "docjson_created": mock_result.docjson is not None
                }
                print("   âœ… Stage 1 (Document Parsing): ì™„ë£Œ")
            except Exception as e:
                stage_results["parsing"] = {"success": False, "error": str(e)}
                print(f"   âŒ Stage 1 (Document Parsing): {e}")

            # Stage 2: Template Matching í…ŒìŠ¤íŠ¸ (DocJSONì´ ìˆëŠ” ê²½ìš°ë§Œ)
            if mock_result.docjson:
                try:
                    await pipeline._stage_template_matching(document_path, preset, mock_result)
                    stage_results["template_matching"] = {
                        "success": "template_matching" in mock_result.stages_completed,
                        "template_found": mock_result.template_match is not None
                    }
                    print("   âœ… Stage 2 (Template Matching): ì™„ë£Œ")
                except Exception as e:
                    stage_results["template_matching"] = {"success": False, "error": str(e)}
                    print(f"   âš ï¸ Stage 2 (Template Matching): {e}")

            # Stage 6: Quality Assessment í…ŒìŠ¤íŠ¸
            try:
                await pipeline._stage_quality_assessment(mock_result)
                stage_results["quality_assessment"] = {
                    "success": "quality_assessment" in mock_result.stages_completed,
                    "quality_score": mock_result.quality_score
                }
                print(f"   âœ… Stage 6 (Quality Assessment): í’ˆì§ˆ ì ìˆ˜ {mock_result.quality_score:.1f}")
            except Exception as e:
                stage_results["quality_assessment"] = {"success": False, "error": str(e)}
                print(f"   âŒ Stage 6 (Quality Assessment): {e}")

            successful_stages = len([s for s in stage_results.values() if s.get("success")])
            total_stages = len(stage_results)

            self.test_results["pipeline_stages"] = {
                "status": "âœ… PASS" if successful_stages > 0 else "âŒ FAIL",
                "successful_stages": f"{successful_stages}/{total_stages}",
                "details": stage_results,
                "final_quality_score": mock_result.quality_score
            }

        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["pipeline_stages"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    async def _test_5_end_to_end_processing(self, document_path: str):
        """5ë‹¨ê³„: ì¢…ë‹¨ê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ 5ë‹¨ê³„: ì¢…ë‹¨ê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        try:
            # ê° ì²˜ë¦¬ ë ˆë²¨ë³„ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
            levels = [ProcessingLevel.BASIC, ProcessingLevel.STANDARD]  # COMPLETEëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ

            e2e_results = {}

            for level in levels:
                print(f"   ğŸ§ª {level.value} ë ˆë²¨ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸...")

                try:
                    start_time = time.time()
                    result = await quick_process(
                        document_path,
                        level,
                        str(self.output_dir)
                    )
                    processing_time = time.time() - start_time

                    e2e_results[level.value] = {
                        "success": result.success,
                        "processing_time": processing_time,
                        "stages_completed": len(result.stages_completed),
                        "quality_score": result.quality_score,
                        "output_files": len(result.output_files),
                        "error": result.error
                    }

                    if result.success:
                        print(f"   âœ… {level.value}: {processing_time:.2f}ì´ˆ, í’ˆì§ˆ {result.quality_score:.1f}/100")
                    else:
                        print(f"   âŒ {level.value}: {result.error}")

                except Exception as e:
                    e2e_results[level.value] = {
                        "success": False,
                        "error": str(e)
                    }
                    print(f"   âŒ {level.value}: {e}")

            successful_levels = len([r for r in e2e_results.values() if r.get("success")])
            total_levels = len(e2e_results)

            self.test_results["end_to_end"] = {
                "status": "âœ… PASS" if successful_levels > 0 else "âŒ FAIL",
                "successful_levels": f"{successful_levels}/{total_levels}",
                "details": e2e_results,
                "average_quality": sum([r.get("quality_score", 0) for r in e2e_results.values() if r.get("success")]) / max(successful_levels, 1)
            }

        except Exception as e:
            print(f"âŒ ì¢…ë‹¨ê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.test_results["end_to_end"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    async def _test_6_output_quality_assessment(self):
        """6ë‹¨ê³„: ì¶œë ¥ í’ˆì§ˆ í‰ê°€"""
        print("ğŸ“Š 6ë‹¨ê³„: ì¶œë ¥ í’ˆì§ˆ í‰ê°€")
        print("-" * 50)

        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
            output_files = list(self.output_dir.glob("*"))
            metadata_files = list(self.output_dir.glob("*.metadata.json"))
            docjson_files = list(self.output_dir.glob("*.docjson"))

            quality_metrics = {
                "total_output_files": len(output_files),
                "metadata_files": len(metadata_files),
                "docjson_files": len(docjson_files),
                "directories_created": len([f for f in output_files if f.is_dir()])
            }

            # íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
            file_integrity = True
            for metadata_file in metadata_files:
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        json.load(f)
                except Exception:
                    file_integrity = False
                    break

            quality_metrics["file_integrity"] = file_integrity

            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = 0
            if quality_metrics["total_output_files"] > 0:
                quality_score += 25
            if quality_metrics["metadata_files"] > 0:
                quality_score += 25
            if quality_metrics["docjson_files"] > 0:
                quality_score += 25
            if quality_metrics["file_integrity"]:
                quality_score += 25

            print(f"   ğŸ“„ ì´ ì¶œë ¥ íŒŒì¼: {quality_metrics['total_output_files']}ê°œ")
            print(f"   ğŸ“‹ ë©”íƒ€ë°ì´í„° íŒŒì¼: {quality_metrics['metadata_files']}ê°œ")
            print(f"   ğŸ“Š DocJSON íŒŒì¼: {quality_metrics['docjson_files']}ê°œ")
            print(f"   ğŸ”’ íŒŒì¼ ë¬´ê²°ì„±: {'âœ…' if file_integrity else 'âŒ'}")
            print(f"   ğŸ“Š ì¶œë ¥ í’ˆì§ˆ ì ìˆ˜: {quality_score}/100")

            self.test_results["output_quality"] = {
                "status": "âœ… PASS" if quality_score >= 50 else "âŒ FAIL",
                "quality_score": quality_score,
                "details": quality_metrics
            }

        except Exception as e:
            print(f"âŒ ì¶œë ¥ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            self.test_results["output_quality"] = {
                "status": "âŒ FAIL",
                "error": str(e)
            }

        print()

    def _generate_comprehensive_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("=" * 70)
        print("ğŸ“‹ ì¢…í•© íŒŒì´í”„ë¼ì¸ ê±´ì „ì„± í‰ê°€ ë³´ê³ ì„œ")
        print("=" * 70)

        total_time = time.time() - self.start_time

        # ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        test_categories = [
            ("ê°œë³„ ëª¨ë“ˆ", "individual_modules"),
            ("íŒŒì„œ ê¸°ëŠ¥", "parser_functionality"),
            ("ì„¤ì • ì‹œìŠ¤í…œ", "configuration_system"),
            ("íŒŒì´í”„ë¼ì¸ ë‹¨ê³„", "pipeline_stages"),
            ("ì¢…ë‹¨ê°„ ì²˜ë¦¬", "end_to_end"),
            ("ì¶œë ¥ í’ˆì§ˆ", "output_quality")
        ]

        passed_tests = 0
        total_tests = len(test_categories)

        print(f"\nğŸ• ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"â° í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        for category_name, category_key in test_categories:
            result = self.test_results.get(category_key, {"status": "âš ï¸ SKIP"})
            status = result.get("status", "â“ UNKNOWN")
            print(f"   {category_name:12s}: {status}")

            if "âœ… PASS" in status:
                passed_tests += 1

        # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
        success_rate = (passed_tests / total_tests) * 100

        print(f"\nğŸ¯ ì „ì²´ ì„±ê³µë¥ : {passed_tests}/{total_tests} ({success_rate:.1f}%)")

        # ì„±ëŠ¥ ì§€í‘œ
        if "end_to_end" in self.test_results and self.test_results["end_to_end"].get("details"):
            avg_quality = self.test_results["end_to_end"].get("average_quality", 0)
            print(f"ğŸ“ˆ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.1f}/100")

        # ë“±ê¸‰ ê²°ì •
        if success_rate >= 90:
            grade = "ğŸ† A+ (ìš°ìˆ˜)"
        elif success_rate >= 80:
            grade = "ğŸ¥‡ A (ì–‘í˜¸)"
        elif success_rate >= 70:
            grade = "ğŸ¥ˆ B (ë³´í†µ)"
        elif success_rate >= 60:
            grade = "ğŸ¥‰ C (ê°œì„  í•„ìš”)"
        else:
            grade = "âŒ F (ë¶ˆëŸ‰)"

        print(f"ğŸ… ì „ì²´ ë“±ê¸‰: {grade}")

        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if success_rate >= 80:
            print("   âœ… íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            print("   âœ… í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if success_rate < 100:
                print("   ğŸ“ˆ ë¯¸ì„¸í•œ ì¡°ì •ìœ¼ë¡œ ì™„ë²½í•œ ì‹œìŠ¤í…œì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif success_rate >= 60:
            print("   âš ï¸ ì‹œìŠ¤í…œì´ ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ğŸ”§ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤ì„ ì ê²€í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.")
        else:
            print("   âŒ ì‹œìŠ¤í…œì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸš¨ ë¬¸ì œ í•´ê²° í›„ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

        # ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
        self._save_detailed_report(total_time, success_rate, grade)

    def _save_detailed_report(self, total_time: float, success_rate: float, grade: str):
        """ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ ì €ì¥"""
        report = {
            "test_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_time": total_time,
                "success_rate": success_rate,
                "grade": grade,
                "output_directory": str(self.output_dir)
            },
            "test_results": self.test_results,
            "system_info": {
                "pipeline_version": "2.0_refactored",
                "test_framework": "comprehensive_pipeline_test",
                "python_version": sys.version
            }
        }

        report_file = self.output_dir / "comprehensive_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ì¢…í•© íŒŒì´í”„ë¼ì¸ ê±´ì „ì„± í…ŒìŠ¤íŠ¸")
    parser.add_argument("--document", "-d", type=str, help="í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ê²½ë¡œ")
    parser.add_argument("--output", "-o", type=str, default="test_pipeline_output", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")

    args = parser.parse_args()

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = ComprehensivePipelineTest(args.output)
    await tester.run_full_test(args.document)


if __name__ == "__main__":
    asyncio.run(main())