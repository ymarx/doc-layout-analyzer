#!/usr/bin/env python3
"""
íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ë„êµ¬
ì²˜ë¦¬ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
"""
import json
import asyncio
from pathlib import Path
from datetime import datetime
from src.core.integrated_pipeline import IntegratedPipeline, PipelineConfig, ProcessingMode

class PipelineQualityAssessment:
    """íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ê¸°"""

    def __init__(self, output_dir: str = "pipeline_output"):
        self.output_dir = Path(output_dir)
        self.assessment_report = {}

    async def comprehensive_assessment(self, document_path: str):
        """ì¢…í•© í’ˆì§ˆ í‰ê°€"""
        print("=" * 60)
        print("ğŸ” íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        print("=" * 60)

        document_name = Path(document_path).name
        self.assessment_report = {
            "document": document_name,
            "timestamp": datetime.now().isoformat(),
            "assessments": {}
        }

        # 1. ê¸°ë³¸ ì²˜ë¦¬ ì„±ëŠ¥ í‰ê°€
        await self._assess_basic_processing(document_path)

        # 2. í…œí”Œë¦¿ ì‹œìŠ¤í…œ í‰ê°€
        await self._assess_template_system(document_path)

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í‰ê°€
        await self._assess_hybrid_system(document_path)

        # 4. ì¶œë ¥ í’ˆì§ˆ í‰ê°€
        self._assess_output_quality()

        # 5. ì „ì²´ ì ìˆ˜ ê³„ì‚°
        self._calculate_overall_score()

        # 6. ë³´ê³ ì„œ ì¶œë ¥
        self._print_assessment_report()

        return self.assessment_report

    async def _assess_basic_processing(self, document_path: str):
        """ê¸°ë³¸ ì²˜ë¦¬ ì„±ëŠ¥ í‰ê°€"""
        print("\nğŸ“Š 1. ê¸°ë³¸ ì²˜ë¦¬ ì„±ëŠ¥ í‰ê°€")
        print("-" * 40)

        pipeline = IntegratedPipeline(str(self.output_dir))
        config = PipelineConfig(
            processing_mode=ProcessingMode.FAST,
            enable_template_matching=False,
            enable_user_annotations=False,
            output_formats=[]
        )

        try:
            start_time = datetime.now()
            result = await pipeline.process_document(document_path, config)
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            basic_assessment = {
                "processing_time": processing_time,
                "success": True,
                "docjson_generated": result.docjson is not None,
                "sections_count": len(result.docjson.sections) if result.docjson else 0,
                "total_blocks": 0
            }

            # ë¸”ë¡ ìˆ˜ ê³„ì‚°
            if result.docjson and result.docjson.sections:
                total_blocks = sum(len(section.blocks) for section in result.docjson.sections)
                basic_assessment["total_blocks"] = total_blocks

            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance_score = min(100, max(0, 100 - processing_time * 10))  # 10ì´ˆ = 0ì 
            structure_score = min(100, basic_assessment["sections_count"] * 10)  # 10ì„¹ì…˜ = 100ì 

            basic_assessment["performance_score"] = performance_score
            basic_assessment["structure_score"] = structure_score
            basic_assessment["overall_score"] = (performance_score + structure_score) / 2

            print(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   ğŸ“‹ ì„¹ì…˜ ìˆ˜: {basic_assessment['sections_count']}ê°œ")
            print(f"   ğŸ“¦ ì´ ë¸”ë¡ ìˆ˜: {basic_assessment['total_blocks']}ê°œ")
            print(f"   ğŸ¯ ì„±ëŠ¥ ì ìˆ˜: {performance_score:.1f}/100")
            print(f"   ğŸ—ï¸ êµ¬ì¡° ì ìˆ˜: {structure_score:.1f}/100")
            print(f"   ğŸ“Š ì „ì²´ ì ìˆ˜: {basic_assessment['overall_score']:.1f}/100")

            self.assessment_report["assessments"]["basic_processing"] = basic_assessment

        except Exception as e:
            print(f"   âŒ ê¸°ë³¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.assessment_report["assessments"]["basic_processing"] = {
                "success": False,
                "error": str(e),
                "overall_score": 0
            }

    async def _assess_template_system(self, document_path: str):
        """í…œí”Œë¦¿ ì‹œìŠ¤í…œ í‰ê°€"""
        print("\nğŸ¯ 2. í…œí”Œë¦¿ ì‹œìŠ¤í…œ í‰ê°€")
        print("-" * 40)

        pipeline = IntegratedPipeline(str(self.output_dir))
        config = PipelineConfig(
            processing_mode=ProcessingMode.FAST,
            enable_template_matching=True,
            auto_apply_template=True,
            template_confidence_threshold=0.6,
            enable_user_annotations=False,
            output_formats=[]
        )

        try:
            result = await pipeline.process_document(document_path, config)

            template_assessment = {
                "template_matched": result.template_match is not None,
                "template_name": result.template_match.template_name if result.template_match else None,
                "confidence": result.template_match.confidence if result.template_match else 0,
                "auto_applied": result.metadata.get('template_applied', False),
                "matched_fields": len(result.template_match.matched_fields) if result.template_match else 0
            }

            # í…œí”Œë¦¿ ì ìˆ˜ ê³„ì‚°
            if template_assessment["template_matched"]:
                confidence_score = template_assessment["confidence"] * 100
                application_score = 100 if template_assessment["auto_applied"] else 50
                field_score = min(100, template_assessment["matched_fields"] * 10)
                template_score = (confidence_score + application_score + field_score) / 3
            else:
                template_score = 0

            template_assessment["overall_score"] = template_score

            print(f"   ğŸ¯ í…œí”Œë¦¿ ë§¤ì¹­: {'âœ…' if template_assessment['template_matched'] else 'âŒ'}")
            if template_assessment["template_matched"]:
                print(f"   ğŸ“ í…œí”Œë¦¿ ì´ë¦„: {template_assessment['template_name']}")
                print(f"   ğŸ“Š ì‹ ë¢°ë„: {template_assessment['confidence']:.1%}")
                print(f"   ğŸ¤– ìë™ ì ìš©: {'âœ…' if template_assessment['auto_applied'] else 'âŒ'}")
                print(f"   ğŸ” ë§¤ì¹­ í•„ë“œ: {template_assessment['matched_fields']}ê°œ")
            print(f"   ğŸ“Š í…œí”Œë¦¿ ì ìˆ˜: {template_score:.1f}/100")

            self.assessment_report["assessments"]["template_system"] = template_assessment

        except Exception as e:
            print(f"   âŒ í…œí”Œë¦¿ í‰ê°€ ì‹¤íŒ¨: {e}")
            self.assessment_report["assessments"]["template_system"] = {
                "success": False,
                "error": str(e),
                "overall_score": 0
            }

    async def _assess_hybrid_system(self, document_path: str):
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í‰ê°€"""
        print("\nğŸ­ 3. í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í‰ê°€")
        print("-" * 40)

        pipeline = IntegratedPipeline(str(self.output_dir))
        config = PipelineConfig(
            processing_mode=ProcessingMode.ENHANCED,
            enable_template_matching=True,
            auto_apply_template=True,
            template_confidence_threshold=0.6,
            enable_user_annotations=True,
            enable_diagrams=True,
            output_formats=[]
        )

        try:
            result = await pipeline.process_document(document_path, config)

            hybrid_assessment = {
                "annotation_generated": result.annotation is not None,
                "total_fields": len(result.annotation.fields) if result.annotation else 0,
                "extracted_values": 0,
                "template_fields": 0,
                "auto_detected_fields": 0
            }

            if result.annotation:
                # ì¶”ì¶œëœ ê°’ ê³„ì‚°
                extracted_values = len([v for v in result.annotation.extracted_values.values() if v])
                hybrid_assessment["extracted_values"] = extracted_values

                # í•„ë“œ ì†ŒìŠ¤ ë¶„ì„
                template_fields = len([f for f in result.annotation.fields
                                     if f.metadata.get('source') == 'template'])
                auto_fields = len([f for f in result.annotation.fields
                                 if f.metadata.get('source') == 'auto_detection'])

                hybrid_assessment["template_fields"] = template_fields
                hybrid_assessment["auto_detected_fields"] = auto_fields

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            field_coverage_score = min(100, hybrid_assessment["total_fields"] * 10)  # 10í•„ë“œ = 100ì 
            extraction_rate = (hybrid_assessment["extracted_values"] / max(1, hybrid_assessment["total_fields"])) * 100
            hybrid_balance_score = 100 if (hybrid_assessment["template_fields"] > 0 and
                                         hybrid_assessment["auto_detected_fields"] > 0) else 75

            hybrid_score = (field_coverage_score + extraction_rate + hybrid_balance_score) / 3
            hybrid_assessment["overall_score"] = hybrid_score

            print(f"   ğŸ“ ì£¼ì„ ìƒì„±: {'âœ…' if hybrid_assessment['annotation_generated'] else 'âŒ'}")
            print(f"   ğŸ“‹ ì´ í•„ë“œ: {hybrid_assessment['total_fields']}ê°œ")
            print(f"   âœ… ì¶”ì¶œ ê°’: {hybrid_assessment['extracted_values']}ê°œ")
            print(f"   ğŸ¯ í…œí”Œë¦¿ í•„ë“œ: {hybrid_assessment['template_fields']}ê°œ")
            print(f"   ğŸ¤– ìë™ê°ì§€ í•„ë“œ: {hybrid_assessment['auto_detected_fields']}ê°œ")
            print(f"   ğŸ“Š ì¶”ì¶œë¥ : {extraction_rate:.1f}%")
            print(f"   ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.1f}/100")

            self.assessment_report["assessments"]["hybrid_system"] = hybrid_assessment

        except Exception as e:
            print(f"   âŒ í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ ì‹¤íŒ¨: {e}")
            self.assessment_report["assessments"]["hybrid_system"] = {
                "success": False,
                "error": str(e),
                "overall_score": 0
            }

    def _assess_output_quality(self):
        """ì¶œë ¥ í’ˆì§ˆ í‰ê°€"""
        print("\nğŸ“ 4. ì¶œë ¥ í’ˆì§ˆ í‰ê°€")
        print("-" * 40)

        output_assessment = {
            "metadata_files": 0,
            "docjson_files": 0,
            "annotation_files": 0,
            "template_files": 0,
            "file_integrity": True
        }

        try:
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
            metadata_files = list(self.output_dir.glob("*.metadata.json"))
            output_assessment["metadata_files"] = len(metadata_files)

            # DocJSON íŒŒì¼ í™•ì¸
            docjson_files = list(self.output_dir.glob("*.docjson"))
            output_assessment["docjson_files"] = len(docjson_files)

            # ì£¼ì„ íŒŒì¼ í™•ì¸
            annotation_dir = self.output_dir / "annotations" / "documents"
            if annotation_dir.exists():
                annotation_files = list(annotation_dir.glob("*.json"))
                output_assessment["annotation_files"] = len(annotation_files)

            # í…œí”Œë¦¿ íŒŒì¼ í™•ì¸
            template_dir = self.output_dir / "annotations" / "templates"
            if template_dir.exists():
                template_files = list(template_dir.glob("*.json"))
                output_assessment["template_files"] = len(template_files)

            # íŒŒì¼ ë¬´ê²°ì„± í™•ì¸
            for metadata_file in metadata_files[:3]:  # ìµœê·¼ 3ê°œ íŒŒì¼ë§Œ ì²´í¬
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        json.load(f)
                except Exception:
                    output_assessment["file_integrity"] = False
                    break

            # ì¶œë ¥ ì ìˆ˜ ê³„ì‚°
            file_diversity_score = min(100, (
                (25 if output_assessment["metadata_files"] > 0 else 0) +
                (25 if output_assessment["docjson_files"] > 0 else 0) +
                (25 if output_assessment["annotation_files"] > 0 else 0) +
                (25 if output_assessment["template_files"] > 0 else 0)
            ))
            integrity_score = 100 if output_assessment["file_integrity"] else 0
            output_score = (file_diversity_score + integrity_score) / 2

            output_assessment["overall_score"] = output_score

            print(f"   ğŸ“„ ë©”íƒ€ë°ì´í„° íŒŒì¼: {output_assessment['metadata_files']}ê°œ")
            print(f"   ğŸ“‹ DocJSON íŒŒì¼: {output_assessment['docjson_files']}ê°œ")
            print(f"   ğŸ“ ì£¼ì„ íŒŒì¼: {output_assessment['annotation_files']}ê°œ")
            print(f"   ğŸ¯ í…œí”Œë¦¿ íŒŒì¼: {output_assessment['template_files']}ê°œ")
            print(f"   ğŸ”’ íŒŒì¼ ë¬´ê²°ì„±: {'âœ…' if output_assessment['file_integrity'] else 'âŒ'}")
            print(f"   ğŸ“Š ì¶œë ¥ í’ˆì§ˆ ì ìˆ˜: {output_score:.1f}/100")

            self.assessment_report["assessments"]["output_quality"] = output_assessment

        except Exception as e:
            print(f"   âŒ ì¶œë ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            self.assessment_report["assessments"]["output_quality"] = {
                "success": False,
                "error": str(e),
                "overall_score": 0
            }

    def _calculate_overall_score(self):
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        assessments = self.assessment_report["assessments"]

        scores = []
        weights = {
            "basic_processing": 0.25,
            "template_system": 0.35,
            "hybrid_system": 0.30,
            "output_quality": 0.10
        }

        total_weight = 0
        weighted_sum = 0

        for component, weight in weights.items():
            if component in assessments and "overall_score" in assessments[component]:
                score = assessments[component]["overall_score"]
                weighted_sum += score * weight
                total_weight += weight

        overall_score = weighted_sum / total_weight if total_weight > 0 else 0
        self.assessment_report["overall_score"] = overall_score

    def _print_assessment_report(self):
        """í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
        print("=" * 60)

        overall_score = self.assessment_report.get("overall_score", 0)

        # ë“±ê¸‰ ê²°ì •
        if overall_score >= 90:
            grade = "ğŸ† A+ (ìš°ìˆ˜)"
        elif overall_score >= 80:
            grade = "ğŸ¥‡ A (ì–‘í˜¸)"
        elif overall_score >= 70:
            grade = "ğŸ¥ˆ B (ë³´í†µ)"
        elif overall_score >= 60:
            grade = "ğŸ¥‰ C (ê°œì„  í•„ìš”)"
        else:
            grade = "âŒ F (ë¶ˆëŸ‰)"

        print(f"\nğŸ¯ ì „ì²´ ì ìˆ˜: {overall_score:.1f}/100")
        print(f"ğŸ… ë“±ê¸‰: {grade}")

        # ì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜
        print(f"\nğŸ“‹ ì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜:")
        assessments = self.assessment_report["assessments"]
        for component, data in assessments.items():
            score = data.get("overall_score", 0)
            component_name = {
                "basic_processing": "ê¸°ë³¸ ì²˜ë¦¬",
                "template_system": "í…œí”Œë¦¿ ì‹œìŠ¤í…œ",
                "hybrid_system": "í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ",
                "output_quality": "ì¶œë ¥ í’ˆì§ˆ"
            }.get(component, component)

            print(f"   {component_name}: {score:.1f}/100")

        # ê²°ê³¼ í•´ì„
        print(f"\nğŸ’¬ í‰ê°€ ê²°ê³¼:")
        if overall_score >= 80:
            print("   âœ… íŒŒì´í”„ë¼ì¸ì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            print("   âœ… í…œí”Œë¦¿ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
            print("   âœ… ë¬¸ì„œ ì²˜ë¦¬ í’ˆì§ˆì´ ì‹¤ìš©ì  ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
        elif overall_score >= 60:
            print("   âš ï¸ íŒŒì´í”„ë¼ì¸ì´ ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸ“ˆ í…œí”Œë¦¿ ì‹œìŠ¤í…œ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("   âŒ íŒŒì´í”„ë¼ì¸ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ğŸ”§ ì‹œìŠ¤í…œ ì ê²€ê³¼ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    assessor = PipelineQualityAssessment()
    document_path = "../ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx"

    if not Path(document_path).exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
        return

    report = await assessor.comprehensive_assessment(document_path)

    # ë³´ê³ ì„œ ì €ì¥
    report_file = Path("pipeline_quality_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")

if __name__ == "__main__":
    asyncio.run(main())