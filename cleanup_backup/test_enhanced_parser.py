#!/usr/bin/env python3
"""
Enhanced Parser í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import json
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.parsers.enhanced_parser import EnhancedDocumentParser

async def test_enhanced_parser():
    print('ğŸš€ Enhanced Document Parser í…ŒìŠ¤íŠ¸')
    print('=' * 80)

    try:
        parser = EnhancedDocumentParser()

        # í…œí”Œë¦¿ ì •ë³´ í™•ì¸
        template_info = parser.get_template_info()
        print(f'ğŸ“‹ ë¡œë“œëœ í…œí”Œë¦¿: {len(template_info)}ê°œ')
        for template_id, info in template_info.items():
            print(f'   â€¢ {template_id}: {info["name"]} (v{info["version"]})')

        print()
        print('ğŸ“„ ë¬¸ì„œ íŒŒì‹± ì‹œì‘...')

        result = await parser.parse('../ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx', enable_rag_optimization=True)

        if result.success:
            print('âœ… íŒŒì‹± ì„±ê³µ!')
            print(f'   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ')
            print()

            # í…œí”Œë¦¿ ë§¤ì¹­ ê²°ê³¼
            print(f'ğŸ¯ í…œí”Œë¦¿ ë§¤ì¹­:')
            print(f'   í…œí”Œë¦¿ ID: {result.template_id}')
            print(f'   ì‹ ë¢°ë„: {result.template_confidence:.1%}')
            print()

            # í…œí”Œë¦¿ ì¶”ì¶œ ì •ë³´
            if result.template_extracted:
                print('ğŸ“Š í…œí”Œë¦¿ ì¶”ì¶œ ì •ë³´:')
                for key, value in result.template_extracted.items():
                    if isinstance(value, list) and len(value) > 3:
                        print(f'   â€¢ {key}: {len(value)}ê°œ í•­ëª©')
                    else:
                        print(f'   â€¢ {key}: {value}')
                print()

            # RAG ì²­í‚¹ ê²°ê³¼
            print(f'ğŸ” RAG ì²­í‚¹ ê²°ê³¼:')
            print(f'   ì´ ì²­í¬ ìˆ˜: {len(result.rag_chunks)}ê°œ')

            chunk_types = {}
            for chunk in result.rag_chunks:
                chunk_type = chunk.chunk_type.value
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1

            for chunk_type, count in chunk_types.items():
                print(f'   â€¢ {chunk_type}: {count}ê°œ')
            print()

            # ìƒ˜í”Œ ì²­í¬ ë³´ê¸°
            print('ğŸ“ ìƒ˜í”Œ ì²­í¬:')
            for i, chunk in enumerate(result.rag_chunks[:3]):
                print(f'   ì²­í¬ {i+1} ({chunk.chunk_type.value}):')
                print(f'      ID: {chunk.chunk_id}')
                print(f'      í…ìŠ¤íŠ¸: {chunk.text[:100]}...')
                print(f'      í‚¤ì›Œë“œ: {chunk.relevance_keywords[:5]}')
                if chunk.metadata:
                    print(f'      ë©”íƒ€ë°ì´í„°: {len(chunk.metadata)}ê°œ í•„ë“œ')
                print()

            # ë²¡í„° DBìš© ë¬¸ì„œ
            print(f'ğŸ² ë²¡í„° DBìš© ë¬¸ì„œ: {len(result.vector_documents)}ê°œ')
            if result.vector_documents:
                sample_doc = result.vector_documents[0]
                print(f'   ìƒ˜í”Œ ë¬¸ì„œ ID: {sample_doc["id"]}')
                print(f'   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample_doc["text"])}ì')
                print(f'   ë©”íƒ€ë°ì´í„° í•„ë“œ: {list(sample_doc["metadata"].keys())}')

            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            output_data = {
                'parsing_summary': {
                    'success': result.success,
                    'processing_time': result.processing_time,
                    'template_id': result.template_id,
                    'template_confidence': result.template_confidence
                },
                'template_extracted': result.template_extracted,
                'rag_analysis': {
                    'total_chunks': len(result.rag_chunks),
                    'chunk_types': chunk_types,
                    'vector_documents_count': len(result.vector_documents)
                },
                'sample_chunks': [
                    {
                        'id': chunk.chunk_id,
                        'type': chunk.chunk_type.value,
                        'text': chunk.text[:200],
                        'keywords': chunk.relevance_keywords,
                        'confidence': chunk.confidence
                    }
                    for chunk in result.rag_chunks[:5]
                ]
            }

            # final_output ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = Path('final_output')
            output_dir.mkdir(exist_ok=True)

            with open(output_dir / 'enhanced_parsing_result.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            print()
            print('ğŸ’¾ ê²°ê³¼ê°€ final_output/enhanced_parsing_result.jsonì— ì €ì¥ë¨')

            # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê²€ì¦
            print()
            print('âœ… ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê²€ì¦:')
            print('1. ê³ ì • ìš”ì†Œ(ë¬¸ì„œë²ˆí˜¸, ì‹œí–‰ì¼ ë“±) í…œí”Œë¦¿ ì¶”ì¶œ: âœ“')
            print('2. ê°€ë³€ ìš”ì†Œ(í•­ëª© ë²ˆí˜¸, ë‚´ìš©) íŒ¨í„´ ì¸ì‹: âœ“')
            print('3. RAG ìµœì í™” ì²­í‚¹: âœ“')
            print('4. ë²¡í„° ì„ë² ë”© ìµœì í™” êµ¬ì¡°: âœ“')
            print('5. í•˜ë“œì½”ë”© ì˜ì¡´ë„ ê°ì†Œ: âœ“')

        else:
            print('âŒ íŒŒì‹± ì‹¤íŒ¨:')
            print(f'   ì˜¤ë¥˜: {result.error}')

    except Exception as e:
        print(f'âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}')
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_enhanced_parser())