#!/usr/bin/env python3
"""
5ë‹¨ê³„: íŒ¨í„´ ì¸ì‹ ë° ìµœì¢… íŒŒì‹± (Pattern Recognition & Final Parsing)
ìƒì„±ëœ í…œí”Œë¦¿ê³¼ íŒ¨í„´ ì¸ì‹ì„ ê²°í•©í•˜ì—¬ ìµœì¢… íŒŒì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import sys
import json
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.enhanced_modernized_pipeline import EnhancedModernizedPipeline
from src.core.simplified_config import PipelineConfig, ProcessingLevel
from src.core.user_annotations import UserAnnotationManager

async def step5_final_parsing():
    print("=" * 70)
    print("ğŸ¯ 5ë‹¨ê³„: íŒ¨í„´ ì¸ì‹ ë° ìµœì¢… íŒŒì‹±")
    print("=" * 70)

    document_path = "../ê¸°ìˆ ê¸°ì¤€_ì˜ˆì‹œ.docx"

    # 1. ëª¨ë“  ê¸°ëŠ¥ì„ í™œì„±í™”í•œ ì™„ì „ íŒŒì‹±
    print("ğŸš€ ì™„ì „ ê¸°ëŠ¥ íŒŒì‹± ì‹œì‘...")
    start_time = time.time()

    pipeline = EnhancedModernizedPipeline(
        output_dir="step5_final_parsing",
        templates_dir="templates/definitions"
    )

    # ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”
    config = PipelineConfig(
        processing_level=ProcessingLevel.COMPLETE,
        override_output_formats=['docjson', 'annotations', 'vectors']
    )

    result = await pipeline.process_document(document_path, config)
    processing_time = time.time() - start_time

    if not result.success:
        print(f"âŒ ìµœì¢… íŒŒì‹± ì‹¤íŒ¨: {result.error}")
        return None

    print(f"âœ… ìµœì¢… íŒŒì‹± ì™„ë£Œ!")
    print(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
    print(f"ğŸ”„ ì™„ë£Œëœ ë‹¨ê³„: {len(result.stages_completed)}ê°œ")
    print(f"ğŸ“Š ë‹¨ê³„ ëª©ë¡: {', '.join(result.stages_completed)}")
    print()

    # 2. í…œí”Œë¦¿ ë§¤ì¹­ ê²°ê³¼ ë¶„ì„
    await analyze_template_matching(result)

    # 3. íŒ¨í„´ ì¸ì‹ ê²°ê³¼ ë¶„ì„
    await analyze_pattern_recognition(result)

    # 4. í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ê²°ê³¼ ë¶„ì„
    await analyze_hybrid_parsing(result)

    # 5. í’ˆì§ˆ í‰ê°€ ë° ê°œì„ ì 
    await analyze_quality_assessment(result)

    # 6. ìµœì¢… ê²°ê³¼ ê²€ì¦
    await verify_final_results(result)

    return {
        'result': result,
        'processing_time': processing_time,
        'template_confidence': result.template_match.confidence if result.template_match else 0,
        'extracted_fields': len(result.template_match.matched_fields) if result.template_match else 0
    }

async def analyze_template_matching(result):
    """í…œí”Œë¦¿ ë§¤ì¹­ ê²°ê³¼ ë¶„ì„"""
    print("ğŸ“‹ í…œí”Œë¦¿ ë§¤ì¹­ ê²°ê³¼ ë¶„ì„:")

    if result.template_match:
        tm = result.template_match
        print(f"   ğŸ¯ ì‚¬ìš©ëœ í…œí”Œë¦¿: {tm.template_id}")
        print(f"   ğŸ“Š ë§¤ì¹­ ì‹ ë¢°ë„: {tm.confidence:.1%}")
        print(f"   ğŸ“ ë°”ìš´ë”©ë°•ìŠ¤ ì •í™•ë„: {tm.bbox_accuracy:.1%}")
        print(f"   ğŸ”§ ë§¤ì¹­ ì „ëµ: {tm.strategy_used.value}")
        print(f"   âœ… ì„±ê³µ í•„ë“œ: {len(tm.matched_fields)}ê°œ")
        print(f"   âŒ ëˆ„ë½ í•„ë“œ: {len(tm.missing_fields)}ê°œ")
        print()

        # ë§¤ì¹­ ë°©ë²•ë³„ ë¶„ë¥˜
        method_counts = {}
        for field_data in tm.matched_fields.values():
            method = field_data.get('method', 'unknown')
            method_counts[method] = method_counts.get(method, 0) + 1

        print("   ğŸ” ë§¤ì¹­ ë°©ë²•ë³„ ë¶„í¬:")
        for method, count in method_counts.items():
            percentage = (count / len(tm.matched_fields)) * 100
            emoji = {
                'unknown': 'ğŸ“',
                'docjson_metadata': 'ğŸ“Š',
                'process_flow': 'ğŸ”„',
                'template_pattern': 'ğŸ¯'
            }.get(method, 'â“')
            print(f"     {emoji} {method}: {count}ê°œ ({percentage:.1f}%)")
        print()
    else:
        print("   âŒ í…œí”Œë¦¿ ë§¤ì¹­ ì‹¤íŒ¨")
        print()

async def analyze_pattern_recognition(result):
    """íŒ¨í„´ ì¸ì‹ ê²°ê³¼ ë¶„ì„"""
    print("ğŸ” íŒ¨í„´ ì¸ì‹ ê²°ê³¼ ë¶„ì„:")

    # DocJSONì—ì„œ íŒ¨í„´ ë¶„ì„
    docjson = result.docjson
    metadata = docjson.get('metadata', {})

    # 1. ë©”íƒ€ë°ì´í„° íŒ¨í„´
    print("   ğŸ“Š ë©”íƒ€ë°ì´í„° íŒ¨í„´ ì¸ì‹:")
    detected_patterns = []

    if metadata.get('document_number'):
        detected_patterns.append(f"ë¬¸ì„œë²ˆí˜¸: {metadata['document_number']}")
    if metadata.get('effective_date'):
        detected_patterns.append(f"íš¨ë ¥ë°œìƒì¼: {metadata['effective_date']}")
    if metadata.get('author'):
        detected_patterns.append(f"ì‘ì„±ì: {metadata['author']}")

    for pattern in detected_patterns:
        print(f"     âœ… {pattern}")

    # 2. í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš° íŒ¨í„´
    process_flows = metadata.get('source', [])
    sequential_flows = [s for s in process_flows if s.get('type') == 'sequential']

    if sequential_flows:
        print(f"\n   ğŸ”„ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš° íŒ¨í„´: {len(sequential_flows)}ê°œ ê°ì§€")
        for flow in sequential_flows:
            steps = flow.get('steps', [])
            print(f"     ìˆœì„œë„: {len(steps)}ë‹¨ê³„")
            for step in steps[:3]:
                print(f"       {step.get('marker', '')} {step.get('title', '')}")
            if len(steps) > 3:
                print(f"       ... ë° {len(steps) - 3}ê°œ ë”")

    # 3. êµ¬ì¡°ì  íŒ¨í„´
    sections = docjson.get('sections', [])
    section_types = {}
    for section in sections:
        section_type = section.get('type', 'unknown')
        section_types[section_type] = section_types.get(section_type, 0) + 1

    print(f"\n   ğŸ—ï¸ êµ¬ì¡°ì  íŒ¨í„´: {len(sections)}ê°œ ì„¹ì…˜")
    for sec_type, count in section_types.items():
        print(f"     â€¢ {sec_type}: {count}ê°œ")
    print()

async def analyze_hybrid_parsing(result):
    """í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ê²°ê³¼ ë¶„ì„"""
    print("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ê²°ê³¼ ë¶„ì„:")

    if result.template_match:
        tm = result.template_match

        # í…œí”Œë¦¿ ê¸°ë°˜ vs ì¶”ë¡  ê¸°ë°˜ ë¶„ë¥˜
        template_based = []
        inference_based = []

        for field_name, field_data in tm.matched_fields.items():
            method = field_data.get('method', 'unknown')
            if method in ['docjson_metadata', 'process_flow']:
                inference_based.append(field_name)
            else:
                template_based.append(field_name)

        print(f"   ğŸ¯ í…œí”Œë¦¿ ê¸°ë°˜ ì¶”ì¶œ: {len(template_based)}ê°œ")
        for field in template_based[:5]:
            print(f"     â€¢ {field}")
        if len(template_based) > 5:
            print(f"     ... ë° {len(template_based) - 5}ê°œ ë”")

        print(f"\n   ğŸ§  ì¶”ë¡  ê¸°ë°˜ ì¶”ì¶œ: {len(inference_based)}ê°œ")
        for field in inference_based:
            print(f"     â€¢ {field}")

        # í•˜ì´ë¸Œë¦¬ë“œ íš¨ìœ¨ì„± ê³„ì‚°
        total_fields = len(tm.matched_fields)
        template_ratio = len(template_based) / total_fields if total_fields > 0 else 0
        inference_ratio = len(inference_based) / total_fields if total_fields > 0 else 0

        print(f"\n   ğŸ“ˆ í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¨:")
        print(f"     í…œí”Œë¦¿ ê¸°ë°˜: {template_ratio:.1%}")
        print(f"     ì¶”ë¡  ê¸°ë°˜: {inference_ratio:.1%}")
        print()

async def analyze_quality_assessment(result):
    """í’ˆì§ˆ í‰ê°€ ë° ê°œì„ ì  ë¶„ì„"""
    print("ğŸ“ˆ í’ˆì§ˆ í‰ê°€ ë° ê°œì„ ì :")

    # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ
    if hasattr(result, 'field_extraction_accuracy'):
        print(f"   ğŸ“Š í•„ë“œ ì¶”ì¶œ ì •í™•ë„: {result.field_extraction_accuracy:.1%}")
    if hasattr(result, 'bbox_improvement_score'):
        print(f"   ğŸ“ ë°”ìš´ë”©ë°•ìŠ¤ ê°œì„  ì ìˆ˜: {result.bbox_improvement_score:.1%}")

    # ê°œì„  ë¶„ì„
    if result.template_match:
        confidence = result.template_match.confidence

        print(f"\n   ğŸ¯ ì„±ëŠ¥ ë¶„ì„:")
        if confidence >= 0.8:
            print(f"     ğŸŸ¢ ìš°ìˆ˜ (ì‹ ë¢°ë„: {confidence:.1%})")
        elif confidence >= 0.6:
            print(f"     ğŸŸ¡ ì–‘í˜¸ (ì‹ ë¢°ë„: {confidence:.1%})")
        else:
            print(f"     ğŸ”´ ê°œì„  í•„ìš” (ì‹ ë¢°ë„: {confidence:.1%})")

        # ê°œì„  ì œì•ˆ
        print(f"\n   ğŸ’¡ ê°œì„  ì œì•ˆ:")
        if len(result.template_match.missing_fields) > 0:
            print(f"     â€¢ ëˆ„ë½ëœ {len(result.template_match.missing_fields)}ê°œ í•„ë“œì— ëŒ€í•œ íŒ¨í„´ ì¶”ê°€")
        if result.template_match.bbox_accuracy < 0.7:
            print(f"     â€¢ ë°”ìš´ë”©ë°•ìŠ¤ ì •í™•ë„ ê°œì„  (í˜„ì¬: {result.template_match.bbox_accuracy:.1%})")
        if confidence < 0.8:
            print(f"     â€¢ í…œí”Œë¦¿ íŒ¨í„´ ì •êµí™” í•„ìš”")
    print()

async def verify_final_results(result):
    """ìµœì¢… ê²°ê³¼ ê²€ì¦"""
    print("âœ… ìµœì¢… ê²°ê³¼ ê²€ì¦:")

    # 1. í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ ê²€ì¦
    essential_fields = ['document_number', 'effective_date', 'title']
    extracted_essential = []

    if result.template_match:
        for field in essential_fields:
            if field in result.template_match.matched_fields:
                value = result.template_match.matched_fields[field].get('value', '')
                if value.strip():
                    extracted_essential.append(field)

    print(f"   ğŸ“‹ í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ: {len(extracted_essential)}/{len(essential_fields)}")
    for field in extracted_essential:
        print(f"     âœ… {field}")

    missing_essential = set(essential_fields) - set(extracted_essential)
    for field in missing_essential:
        print(f"     âŒ {field} (ëˆ„ë½)")

    # 2. íŒŒì¼ ìƒì„± ê²€ì¦
    output_files = result.output_files if hasattr(result, 'output_files') else {}
    print(f"\n   ğŸ“ ìƒì„±ëœ íŒŒì¼: {len(output_files)}ê°œ")
    for file_type, file_path in output_files.items():
        if file_path.exists():
            size = file_path.stat().st_size
            print(f"     âœ… {file_type}: {file_path.name} ({size:,} bytes)")
        else:
            print(f"     âŒ {file_type}: íŒŒì¼ ì—†ìŒ")

    # 3. ì¢…í•© í‰ê°€
    print(f"\n   ğŸ† ì¢…í•© í‰ê°€:")

    total_score = 0
    max_score = 0

    # í…œí”Œë¦¿ ë§¤ì¹­ ì ìˆ˜ (40ì )
    if result.template_match:
        template_score = result.template_match.confidence * 40
        total_score += template_score
        print(f"     í…œí”Œë¦¿ ë§¤ì¹­: {template_score:.1f}/40ì ")
    max_score += 40

    # í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ ì ìˆ˜ (30ì )
    essential_score = (len(extracted_essential) / len(essential_fields)) * 30
    total_score += essential_score
    print(f"     í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ: {essential_score:.1f}/30ì ")
    max_score += 30

    # íŒŒì¼ ìƒì„± ì ìˆ˜ (20ì )
    file_score = (len([f for f in output_files.values() if f.exists()]) / max(len(output_files), 1)) * 20
    total_score += file_score
    print(f"     íŒŒì¼ ìƒì„±: {file_score:.1f}/20ì ")
    max_score += 20

    # ì²˜ë¦¬ ì†ë„ ì ìˆ˜ (10ì )
    processing_time = getattr(result, 'processing_time', 0)
    speed_score = max(0, min(10, 10 - processing_time)) if processing_time > 0 else 10
    total_score += speed_score
    print(f"     ì²˜ë¦¬ ì†ë„: {speed_score:.1f}/10ì ")
    max_score += 10

    final_percentage = (total_score / max_score) * 100 if max_score > 0 else 0
    print(f"\n   ğŸ¯ ìµœì¢… ì ìˆ˜: {total_score:.1f}/{max_score}ì  ({final_percentage:.1f}%)")

    if final_percentage >= 80:
        print("     ğŸŸ¢ ìš°ìˆ˜ - í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥")
    elif final_percentage >= 60:
        print("     ğŸŸ¡ ì–‘í˜¸ - ì¼ë¶€ ê°œì„  í›„ ì‚¬ìš© ê¶Œì¥")
    else:
        print("     ğŸ”´ ê°œì„  í•„ìš” - ì¶”ê°€ ê°œë°œ í•„ìš”")
    print()

async def demonstrate_advanced_features():
    """ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨"""
    print("ğŸš€ ê³ ê¸‰ ê¸°ëŠ¥ ë° í–¥í›„ ê³„íš:")
    print()

    advanced_features = [
        "ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ íŒ¨í„´ í•™ìŠµ",
        "ğŸ“Š í…œí”Œë¦¿ ì„±ëŠ¥ ë¶„ì„ ë° ìë™ ìµœì í™”",
        "ğŸŒ ë‹¤êµ­ì–´ ë¬¸ì„œ ì§€ì› (ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´)",
        "ğŸ“± ì›¹ ê¸°ë°˜ ì‹œê°ì  í¸ì§‘ ì¸í„°í˜ì´ìŠ¤",
        "ğŸ”„ ì‹¤ì‹œê°„ ë¬¸ì„œ ì²˜ë¦¬ API",
        "ğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ ë° ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì§€ì›",
        "ğŸ§  ì‚¬ìš©ì í–‰ë™ í•™ìŠµ ë° ê°œì¸í™”",
        "ğŸ” ë³´ì•ˆ ë° ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬",
        "ğŸ“‹ ì›Œí¬í”Œë¡œìš° ìë™í™”",
        "ğŸ¯ ì—…ê³„ë³„ íŠ¹í™” í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬"
    ]

    for feature in advanced_features:
        print(f"   {feature}")

    print("\nğŸ’¡ ìµœì í™” íŒ:")
    tips = [
        "í…œí”Œë¦¿ íŒ¨í„´ì„ ì •êµí•˜ê²Œ ì‘ì„±í• ìˆ˜ë¡ ì •í™•ë„ í–¥ìƒ",
        "ë°”ìš´ë”©ë°•ìŠ¤ ì •ë³´ë¥¼ ì¶”ê°€í•˜ë©´ ì¶”ì¶œ ì†ë„ ê°œì„ ",
        "ë¬¸ì„œë³„ íŠ¹ì„±ì— ë§ëŠ” ì „ìš© í…œí”Œë¦¿ ìƒì„± ê¶Œì¥",
        "ì •ê¸°ì ì¸ í…œí”Œë¦¿ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—…ë°ì´íŠ¸",
        "ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì§€ì†ì  ê°œì„ "
    ]

    for i, tip in enumerate(tips, 1):
        print(f"   {i}. {tip}")

if __name__ == "__main__":
    final_info = asyncio.run(step5_final_parsing())

    if final_info:
        print("\n" + "="*70)
        print("ğŸ‰ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {final_info['processing_time']:.3f}ì´ˆ")
        print(f"ğŸ¯ í…œí”Œë¦¿ ì‹ ë¢°ë„: {final_info['template_confidence']:.1%}")
        print(f"ğŸ“Š ì¶”ì¶œëœ í•„ë“œ: {final_info['extracted_fields']}ê°œ")
        print("="*70)

        # ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨
        print("\nğŸš€ ì‹œìŠ¤í…œ ê³ ë„í™” ë°©í–¥:")
        asyncio.run(demonstrate_advanced_features())